{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Preprocess the Text Data (Title Column)\n",
    "\n",
    "Tokenization: Split each article title into individual words.\n",
    "Vocabulary Creation: Create a vocabulary from all unique words in the titles and assign each word a unique ID.\n",
    "Padding: Ensure each title has the same length by padding shorter titles or truncating longer ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Parquet file\n",
    "dataset_path_parquet = \"/dtu/blackhole/1e/203774/final_df_new.parquet\"\n",
    "df = pd.read_parquet(dataset_path_parquet, engine='pyarrow')\n",
    "\n",
    "# Extract titles for tokenization\n",
    "titles = df['title'].astype(str).tolist()\n",
    "\n",
    "# Step 1: Tokenize titles using BPE with reduced vocab size\n",
    "# Save titles to a temporary file to train BPE model\n",
    "with open(\"titles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for title in titles:\n",
    "        f.write(title + \"\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, List\n",
    "import torch\n",
    "import tokenizers\n",
    "import rich\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_vectors(filename=\"glove.6B.300d.txt\") -> Tuple[List[str], torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Load the GloVe vectors and parse vocabulary and vectors.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of the GloVe file to load.\n",
    "\n",
    "    Returns:\n",
    "        vocabulary (List[str]): List of words in the GloVe vocabulary.\n",
    "        vectors (torch.Tensor): Tensor of embedding vectors.\n",
    "    \"\"\"\n",
    "    # Download GloVe file from Hugging Face hub\n",
    "    path = Path(hf_hub_download(repo_id=\"stanfordnlp/glove\", filename=\"glove.6B.zip\"))\n",
    "    target_file = path.parent / filename\n",
    "\n",
    "    # Extract GloVe file if not already extracted\n",
    "    if not target_file.exists():\n",
    "        with zipfile.ZipFile(path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(path.parent)\n",
    "\n",
    "        # Ensure target file exists after extraction\n",
    "        if not target_file.exists():\n",
    "            print(\"Available files:\")\n",
    "            for p in path.parent.iterdir():\n",
    "                print(p)\n",
    "            raise ValueError(\n",
    "                f\"Target file `{target_file.name}` can't be found. Check if `{filename}` was properly downloaded.\"\n",
    "            )\n",
    "\n",
    "    # Parse vocabulary and vectors\n",
    "    vocabulary = []\n",
    "    vectors = []\n",
    "    with open(target_file, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in tqdm(f.readlines(), desc=f\"Parsing {target_file.name}...\"):\n",
    "            word, *vector = line.split()\n",
    "            vocabulary.append(word)\n",
    "            vectors.append(torch.tensor([float(v) for v in vector], dtype=torch.float32))\n",
    "    vectors = torch.stack(vectors)\n",
    "    return vocabulary, vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing glove.6B.300d.txt...: 100%|██████████| 400001/400001 [00:41<00:00, 9661.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">glove_vocabulary: <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400001</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "glove_vocabulary: \u001b[33mtype\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mlength\u001b[0m=\u001b[1;36m400001\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">glove_vectors: <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.Tensor'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400001</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "glove_vectors: \u001b[33mtype\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.Tensor'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mshape\u001b[0m=\u001b[1;35mtorch\u001b[0m\u001b[1;35m.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m400001\u001b[0m, \u001b[1;36m300\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Prepare data\n",
    "glove_vocabulary, glove_vectors = load_glove_vectors()\n",
    "rich.print(f\"glove_vocabulary: type={type(glove_vocabulary)}, length={len(glove_vocabulary)}\")\n",
    "rich.print(f\"glove_vectors: type={type(glove_vectors)}, shape={glove_vectors.shape}, dtype={glove_vectors.dtype}\")\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = ['<|start|>', '<|unknown|>', '<|pad|>']\n",
    "glove_vocabulary = special_tokens + glove_vocabulary\n",
    "glove_vectors = torch.cat([torch.randn(len(special_tokens), glove_vectors.shape[1]), glove_vectors])\n",
    "\n",
    "# Tokenizer for GloVe\n",
    "glove_tokenizer = tokenizers.Tokenizer(\n",
    "    tokenizers.models.WordLevel(vocab={v: i for i, v in enumerate(glove_vocabulary)}, unk_token=\"<|unknown|>\")\n",
    ")\n",
    "glove_tokenizer.normalizer = tokenizers.normalizers.BertNormalizer(strip_accents=False)\n",
    "glove_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize titles using GloVe tokenizer\n",
    "encoded_titles = [glove_tokenizer.encode(title).ids for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304],\n",
       " [687, 8452, 12233, 1, 649, 37972, 730, 1304]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Length: 8.0\n",
      "Median Length: 8.0\n",
      "90th Percentile: 8.0\n",
      "95th Percentile: 8.0\n",
      "Selected Max Length: 8\n",
      "Number of truncated titles: 0 (0.00%)\n",
      "Training set shape: torch.Size([80, 8]), Validation set shape: torch.Size([20, 8])\n"
     ]
    }
   ],
   "source": [
    "# # Step 1: Calculate the length of each tokenized title\n",
    "# title_lengths = [len(title) for title in encoded_titles]\n",
    "\n",
    "# # Step 2: Analyze the distribution of lengths\n",
    "# mean_length = np.mean(title_lengths)\n",
    "# median_length = np.median(title_lengths)\n",
    "# percentile_90 = np.percentile(title_lengths, 90)  # 90th percentile\n",
    "# percentile_95 = np.percentile(title_lengths, 95)  # 95th percentile\n",
    "\n",
    "# # Step 3: Choose a max_length based on the distribution (e.g., 90th percentile)\n",
    "# max_length = int(percentile_90)  # Adjust this based on your requirements\n",
    "\n",
    "# print(f\"Mean Length: {mean_length}\")\n",
    "# print(f\"Median Length: {median_length}\")\n",
    "# print(f\"90th Percentile: {percentile_90}\")\n",
    "# print(f\"95th Percentile: {percentile_95}\")\n",
    "# print(f\"Selected Max Length: {max_length}\")\n",
    "\n",
    "# # Step 4: Pad or truncate the titles based on the selected max_length\n",
    "# padded_titles = torch.zeros((len(encoded_titles), max_length), dtype=torch.long)\n",
    "# num_truncated = 0\n",
    "\n",
    "# for i, encoded in enumerate(encoded_titles):\n",
    "#     length = min(len(encoded), max_length)\n",
    "#     padded_titles[i, :length] = torch.tensor(encoded[:length])\n",
    "#     if len(encoded) > max_length:\n",
    "#         num_truncated += 1\n",
    "\n",
    "# print(f\"Number of truncated titles: {num_truncated} ({num_truncated / len(encoded_titles) * 100:.2f}%)\")\n",
    "\n",
    "# # Step 5: Split the dataset into training and validation sets\n",
    "# train_titles, val_titles = train_test_split(padded_titles, test_size=0.2, random_state=42)\n",
    "# print(f\"Training set shape: {train_titles.shape}, Validation set shape: {val_titles.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(title_lengths, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "# plt.axvline(12, color='red', linestyle='dashed', label='Selected Max Length (12)')\n",
    "# plt.axvline(mean_length, color='green', linestyle='dashed', label='Mean Length')\n",
    "# plt.axvline(median_length, color='orange', linestyle='dashed', label='Median Length')\n",
    "# plt.title(\"Distribution of Tokenized Title Lengths\")\n",
    "# plt.xlabel(\"Title Length\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# num_truncated = sum(1 for length in title_lengths if length > 12)\n",
    "# print(f\"Number of truncated titles: {num_truncated} ({num_truncated / len(title_lengths) * 100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine the maximum length dynamically\n",
    "# max_length = int(np.percentile([len(title) for title in encoded_titles], 90))  # Use 90th percentile\n",
    "\n",
    "# # Pad or truncate the tokenized titles\n",
    "# padded_titles = torch.zeros((len(encoded_titles), max_length), dtype=torch.long)\n",
    "# for i, encoded in enumerate(encoded_titles):\n",
    "#     length = min(len(encoded), max_length)\n",
    "#     padded_titles[i, :length] = torch.tensor(encoded[:length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamically determined max length: 8\n",
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the titles (fill NaNs and tokenize)\n",
    "titles = df['title'].fillna('').tolist()  # fills any missing values with an empty string and converts the column into a list of strings \n",
    "encoded_titles = [glove_tokenizer.encode(title).ids for title in titles] # tokenizes each title into a sequence of integers using the glove tokenizer \n",
    "\n",
    "# Step 2: Dynamically determine max length\n",
    "max_length = max(len(encoded) for encoded in encoded_titles)  # finds the length of the longest tokenized title in encoded titles \n",
    "print(f\"Dynamically determined max length: {max_length}\") # dynamically adjusting the max length ensures that the padding truncation step can handle all sequences without manually specifying a dixed length \n",
    "\n",
    "# Step 3: Pad or truncate the tokenized titles\n",
    "padded_titles = torch.zeros((len(encoded_titles), max_length), dtype=torch.long)  # creates a pytorch tensor initialized with zeros , the shape of the tensor is number of titles , max length \n",
    "for i, encoded in enumerate(encoded_titles):\n",
    "    length = min(len(encoded), max_length)\n",
    "    padded_titles[i, :length] = torch.tensor(encoded[:length])\n",
    "\n",
    "# Step 4: Split into train and validation sets\n",
    "train_data, val_data = train_test_split(padded_titles, test_size=0.2, random_state=42) # splits the padded titles into two subsets , train data used for training the model and validation data used for validating the model during training \n",
    "# ensures the model is trained on one portion of the data and evaluated on separate unseen portion to prevent overfitting \n",
    "\n",
    "# Step 5: Convert to PyTorch tensors\n",
    "train_data = train_data.clone().detach() \n",
    "val_data = val_data.clone().detach()\n",
    "\n",
    "print(\"Data preparation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Encoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Word Embedding Layer\n",
    "class WordEmbeddings(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(WordEmbeddings, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embeddings=torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=True  # Set to False to fine-tune embeddings\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        return embeddings\n",
    "\n",
    "# Self-Attention Layer\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Multi-Head Self-Attention based on the provided formulas.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimension of the input.\n",
    "            num_heads (int): Number of self-attention heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Separate Query and Value projection matrices for each head\n",
    "        self.query_matrices = nn.Parameter(torch.randn(num_heads, self.head_dim, self.head_dim))\n",
    "        self.value_matrices = nn.Parameter(torch.randn(num_heads, self.head_dim, self.head_dim))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_len, embed_dim).\n",
    "\n",
    "        Returns:\n",
    "            output: Tensor of shape (batch_size, seq_len, embed_dim).\n",
    "            attention_weights: Tensor of shape (batch_size, num_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = inputs.size()\n",
    "\n",
    "        # Reshape input for multi-head computation\n",
    "        inputs = inputs.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        inputs = inputs.permute(0, 2, 1, 3)  # Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        attention_outputs = []\n",
    "        attention_weights_all_heads = []\n",
    "\n",
    "        for k in range(self.num_heads):\n",
    "            # Compute attention scores for head k\n",
    "            Q_k = self.query_matrices[k]  # Shape: (head_dim, head_dim)\n",
    "            attention_scores = torch.einsum('bqi,qk,bqj->bij', inputs[:, k, :, :], Q_k, inputs[:, k, :, :])\n",
    "            attention_weights = F.softmax(attention_scores, dim=-1)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "            # Compute weighted sum for head k\n",
    "            weighted_sum = torch.einsum('bij,bjd->bid', attention_weights, inputs[:, k, :, :])  # (batch_size, seq_len, head_dim)\n",
    "\n",
    "            # Apply value projection\n",
    "            V_k = self.value_matrices[k]  # Shape: (head_dim, head_dim)\n",
    "            weighted_output = torch.einsum('bqd,dk->bqk', weighted_sum, V_k)  # Shape: (batch_size, seq_len, head_dim)\n",
    "\n",
    "            attention_outputs.append(weighted_output)\n",
    "            attention_weights_all_heads.append(attention_weights)\n",
    "\n",
    "        # Concatenate outputs from all heads\n",
    "        concat_output = torch.cat(attention_outputs, dim=-1)  # Shape: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Stack attention weights for all heads\n",
    "        attention_weights_all_heads = torch.stack(attention_weights_all_heads, dim=1)  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        return concat_output, attention_weights_all_heads\n",
    "\n",
    "\n",
    "class AdditiveWordAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Additive Word Attention for selecting important words\n",
    "    to learn a more informative representation of news titles.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, query_dim):\n",
    "        \"\"\"\n",
    "        Additive Word Attention Network.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings (h_w).\n",
    "            query_dim (int): Dimensionality of the query vector (q_w).\n",
    "        \"\"\"\n",
    "        super(AdditiveWordAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.query_dim = query_dim\n",
    "\n",
    "        # Learnable parameters for attention\n",
    "        self.Vw = nn.Linear(embed_dim, query_dim, bias=True)  # Linear projection matrix\n",
    "        self.vw = nn.Parameter(torch.randn(query_dim))        # Bias vector vw\n",
    "        self.qw = nn.Parameter(torch.randn(query_dim))        # Query vector qw\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute word-level additive attention.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Tensor of shape (batch_size, seq_len, embed_dim), \n",
    "                                   contextual word representations (h_w).\n",
    "\n",
    "        Returns:\n",
    "            weighted_sum (torch.Tensor): Tensor of shape (batch_size, embed_dim), \n",
    "                                         final news representation as a weighted sum.\n",
    "            attention_weights (torch.Tensor): Tensor of shape (batch_size, seq_len), \n",
    "                                              attention weights for each word.\n",
    "        \"\"\"\n",
    "        # Step 1: Linear projection to transform inputs into attention space\n",
    "        projection = self.Vw(inputs)  # Shape: (batch_size, seq_len, query_dim)\n",
    "\n",
    "        # Step 2: Add the bias vector (vw) to the projection\n",
    "        projection_with_bias = projection + self.vw  # Shape: (batch_size, seq_len, query_dim)\n",
    "\n",
    "        # Step 3: Apply tanh activation to introduce non-linearity\n",
    "        scores = torch.tanh(projection_with_bias)  # Shape: (batch_size, seq_len, query_dim)\n",
    "\n",
    "        # Step 4: Compute raw attention scores via dot product with the query vector (qw)\n",
    "        attention_scores = torch.matmul(scores, self.qw)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Step 5: Normalize the attention scores to probabilities using softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Step 6: Compute the weighted sum of the input embeddings based on attention weights\n",
    "        weighted_sum = torch.sum(inputs * attention_weights.unsqueeze(-1), dim=1)  # Shape: (batch_size, embed_dim)\n",
    "\n",
    "        # Return the final weighted representation and the attention weights\n",
    "        return weighted_sum, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim, num_heads, query_dim, dropout=0.2):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.additive_attention = nn.Sequential(\n",
    "            nn.Linear(embed_dim, query_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(query_dim, 1),\n",
    "        )\n",
    "        self.reconstruction_head = nn.Linear(embed_dim, embedding_matrix.size(1))  # To reconstruct embeddings\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.word_embeddings(inputs)  # Shape: (batch_size, seq_len, embed_dim)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        attn_output, attn_weights = self.self_attention(x, x, x)\n",
    "        attn_scores = self.additive_attention(attn_output).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        news_representation = torch.sum(attn_weights.unsqueeze(-1) * attn_output, dim=1)\n",
    "\n",
    "        reconstructed = self.reconstruction_head(news_representation)  # Reconstruct embeddings\n",
    "        return reconstructed, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewsEncoder initialized with GloVe embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Convert GloVe embeddings to PyTorch tensor\n",
    "embedding_matrix = glove_vectors  # Assuming `glove_vectors` contains your embedding matrix\n",
    "embed_dim = embedding_matrix.shape[1]\n",
    "\n",
    "# Initialize NewsEncoder\n",
    "num_heads = 15\n",
    "query_vector_dim = 200\n",
    "dropout = 0.2\n",
    "\n",
    "news_encoder = NewsEncoder(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    query_dim=query_vector_dim,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "print(\"NewsEncoder initialized with GloVe embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Testing and Validation\n",
    "Input Data: Pass some titles through the News Encoder.\n",
    "Output: Verify that the encoder produces a consistent representation (vector) for each title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # Assuming train_data is your tokenized and padded dataset\n",
    "# batch_size = 64  # Batch size\n",
    "# epochs = 20  # Number of epochs\n",
    "\n",
    "# # Create DataLoader\n",
    "# train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Loss function and optimizer\n",
    "# criterion = nn.MSELoss()  # Replace this if needed for unsupervised learning\n",
    "# optimizer = torch.optim.Adam(user_encoder.parameters(), lr=1e-3)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(epochs):\n",
    "#     user_encoder.train()  # Set the model to training mode\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for batch_idx, batch in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs = batch[0].to(torch.long)  # Ensure inputs are LongTensor\n",
    "#         representations, attention_weights = user_encoder(inputs)\n",
    "\n",
    "#         # Compute loss (reconstruction, contrastive, etc.)\n",
    "#         loss = criterion(representations, user_encoder.word_embeddings(inputs).mean(dim=1))  # Example reconstruction task\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Accumulate loss\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Print batch progress\n",
    "#         print(\n",
    "#             f\"Epoch {epoch + 1}/{epochs} - Batch {batch_idx + 1}/{len(train_loader)}, \"\n",
    "#             f\"Loss: {loss.item():.4f}\"\n",
    "#         )\n",
    "\n",
    "#         # Visualize attention map for the first batch\n",
    "#         if batch_idx == 0 and epoch == 0:\n",
    "#             # Reshape the attention weights for visualization\n",
    "#             reshaped_weights = attention_weights[0].unsqueeze(0)  # Shape: (1, 20)\n",
    "\n",
    "#             # Plot the heatmap\n",
    "#             plt.figure(figsize=(10, 6))\n",
    "#             sns.heatmap(reshaped_weights.cpu().detach().numpy(), cmap=\"viridis\", annot=True)\n",
    "#             plt.title(\"Attention Weights for First Sample\")\n",
    "#             plt.xlabel(\"Sequence Length\")\n",
    "#             plt.ylabel(\"Attention Heads (Single Row)\")\n",
    "#             plt.show()\n",
    "\n",
    "\n",
    "#     # Print epoch loss\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Titles Shape: torch.Size([5, 8])\n",
      "News Representations Shape: torch.Size([5, 300])\n",
      "Attention Weights Shape: torch.Size([5, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the first 5 rows of the train_titles tensor\n",
    "sample_titles = train_data[:5]  # Slice the first 5 rows\n",
    "\n",
    "# Pass the sample titles through the News Encoder\n",
    "news_representations, attention_weights = news_encoder(sample_titles)\n",
    "\n",
    "# Display the output shapes\n",
    "print(\"Sample Titles Shape:\", sample_titles.shape)  # Expected shape: (5, max_length)\n",
    "print(\"News Representations Shape:\", news_representations.shape)  # Expected shape: (5, embed_dim)\n",
    "print(\"Attention Weights Shape:\", attention_weights.shape)  # Expected shape: (5, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights (as Table):\n",
      "     Word 1    Word 2    Word 3    Word 4    Word 5    Word 6    Word 7  \\\n",
      "0  0.125864  0.125133  0.124144  0.122410  0.125003  0.125730  0.126432   \n",
      "1  0.125766  0.125920  0.125110  0.121812  0.125160  0.125389  0.125510   \n",
      "2  0.125309  0.125476  0.124400  0.122794  0.124897  0.125673  0.125564   \n",
      "3  0.125814  0.125859  0.124665  0.120811  0.125135  0.125010  0.126634   \n",
      "4  0.125539  0.125196  0.125092  0.122263  0.124958  0.125549  0.125859   \n",
      "\n",
      "     Word 8  \n",
      "0  0.125285  \n",
      "1  0.125334  \n",
      "2  0.125887  \n",
      "3  0.126071  \n",
      "4  0.125544  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert attention weights to a DataFrame for readability\n",
    "attention_df = pd.DataFrame(attention_weights.detach().numpy())\n",
    "\n",
    "# Add column headers to identify word positions\n",
    "attention_df.columns = [f\"Word {i+1}\" for i in range(max_length)]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Attention Weights (as Table):\")\n",
    "print(attention_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights Sum (per sequence):\n",
      " [1.0000001  1.0000001  0.99999994 0.9999999  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Sum the attention weights across the sequence dimension (axis=1)\n",
    "attention_sum = torch.sum(attention_weights, dim=1)  # Shape: (batch_size, 1)\n",
    "\n",
    "# Print the result to verify it sums to ~1 for each sequence in the batch\n",
    "print(\"Attention Weights Sum (per sequence):\\n\", attention_sum.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between Title 1 and Title 2 representations: 0.8735371828079224\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Select two representations to compare\n",
    "vector1 = news_representations[3]\n",
    "vector2 = news_representations[4]\n",
    "\n",
    "# Compute cosine similarity using PyTorch\n",
    "similarity = cosine_similarity(vector1.unsqueeze(0), vector2.unsqueeze(0)).item()\n",
    "print(\"Cosine similarity between Title 1 and Title 2 representations:\", similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Encoder\n",
    "\n",
    "Step 1: Input Data Preparation\n",
    "Input: A batch of news representations (output from the News Encoder) for all news articles browsed by users.\n",
    "The input shape will be (batch_size, num_articles, embed_dim), where:\n",
    "num_articles: Number of news articles browsed by the user.\n",
    "embed_dim: The dimension of each news representation (from the News Encoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Multi-Head Self-Attention Layer\n",
    "Apply multi-head self-attention to capture relationships between the user's browsed news articles.\n",
    "Process:\n",
    "Compute attention scores between each pair of articles.\n",
    "Learn contextualized news representations by aggregating information across related articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Additive News Attention Layer\n",
    "Assign importance weights to each news article using an additive attention mechanism.\n",
    "Compute a weighted sum of the news representations based on these attention weights.\n",
    "Output: A single dense vector for each user, representing their preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Define the User Encoder Class\n",
    "Integrate the multi-head self-attention and additive attention layers into a single class.\n",
    "Handle the processing of input data (news representations) and return the user representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements multi-head additive attention for user encoding,\n",
    "    strictly following the provided formulas.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - embed_dim: Dimensionality of the input embeddings (news representations).\n",
    "        - num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAdditiveAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Parameters for each head\n",
    "        self.Q_n = nn.ParameterList([nn.Parameter(torch.randn(self.head_dim, self.head_dim)) for _ in range(num_heads)])\n",
    "        self.V_n = nn.ParameterList([nn.Parameter(torch.randn(self.head_dim, self.head_dim)) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, news_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - news_embeddings: Tensor of shape (batch_size, num_news, embed_dim),\n",
    "                           representing the news representations.\n",
    "\n",
    "        Returns:\n",
    "        - enhanced_news_embeddings: Tensor of shape (batch_size, num_news, embed_dim),\n",
    "                                    enhanced news representations.\n",
    "        - attention_weights: List of tensors of shape (batch_size, num_news, num_news) per head,\n",
    "                             attention weights for each head.\n",
    "        \"\"\"\n",
    "        batch_size, num_news, embed_dim = news_embeddings.size()\n",
    "\n",
    "        # Split embeddings for each head\n",
    "        news_per_head = news_embeddings.view(batch_size, num_news, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        all_head_outputs = []\n",
    "        all_attention_weights = []\n",
    "\n",
    "        for h in range(self.num_heads):\n",
    "            Q_n = self.Q_n[h]\n",
    "            scores = torch.einsum('bnd,dk,bmk->bnm', news_per_head[:, h, :, :], Q_n, news_per_head[:, h, :, :])\n",
    "            attention_weights = F.softmax(scores, dim=-1)\n",
    "            V_n = self.V_n[h]\n",
    "            head_output = torch.einsum('bnm,bmd,dk->bnd', attention_weights, news_per_head[:, h, :, :], V_n)\n",
    "\n",
    "            all_head_outputs.append(head_output)\n",
    "            all_attention_weights.append(attention_weights)\n",
    "\n",
    "        concat_output = torch.cat(all_head_outputs, dim=-1)\n",
    "        return concat_output, all_attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced News Embeddings Shape: torch.Size([4, 10, 300])\n",
      "Attention Weights Shape (per head): torch.Size([4, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test configuration\n",
    "batch_size = 4\n",
    "num_news = 10\n",
    "embed_dim = 300\n",
    "num_heads = 10\n",
    "\n",
    "# Simulated input\n",
    "news_embeddings = torch.randn(batch_size, num_news, embed_dim)\n",
    "\n",
    "# Initialize and test the module\n",
    "attention_layer = MultiHeadAdditiveAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "enhanced_news_embeddings, attention_weights = attention_layer(news_embeddings)\n",
    "\n",
    "# Validate outputs\n",
    "print(\"Enhanced News Embeddings Shape:\", enhanced_news_embeddings.shape)  # Expected: (batch_size, num_news, embed_dim)\n",
    "print(\"Attention Weights Shape (per head):\", attention_weights[0].shape)  # Expected: (batch_size, num_news, num_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UserAdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements additive attention for user encoding based on the provided formulas.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - embed_dim: Dimensionality of the input embeddings (news representations).\n",
    "        \"\"\"\n",
    "        super(UserAdditiveAttention, self).__init__()\n",
    "        self.V_n = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_n = nn.Parameter(torch.zeros(embed_dim))\n",
    "        self.q_n = nn.Parameter(torch.randn(embed_dim))\n",
    "\n",
    "    def forward(self, news_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - news_embeddings: Tensor of shape (batch_size, num_news, embed_dim),\n",
    "                           representing the news representations.\n",
    "\n",
    "        Returns:\n",
    "        - user_representation: Tensor of shape (batch_size, embed_dim),\n",
    "                               the final user representation.\n",
    "        - attention_weights: Tensor of shape (batch_size, num_news),\n",
    "                             attention weights for the news articles.\n",
    "        \"\"\"\n",
    "        transformed_news = self.V_n(news_embeddings)\n",
    "        scores = torch.tanh(transformed_news + self.v_n)\n",
    "        scores = torch.einsum('bnd,d->bn', scores, self.q_n)\n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        user_representation = torch.einsum('bn,bnd->bd', attention_weights, news_embeddings)\n",
    "        return user_representation, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User representation shape: torch.Size([16, 300])\n",
      "Attention weights shape: torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 16\n",
    "num_news = 10  # Number of news articles browsed by each user\n",
    "embed_dim = 300\n",
    "\n",
    "# Dummy input\n",
    "news_embeddings = torch.rand(batch_size, num_news, embed_dim)  # Random news embeddings\n",
    "\n",
    "# Initialize the UserAdditiveAttention layer\n",
    "user_attention_layer = UserAdditiveAttention(embed_dim=embed_dim)\n",
    "\n",
    "# Forward pass\n",
    "user_representation, attention_weights = user_attention_layer(news_embeddings)\n",
    "\n",
    "# Output shapes\n",
    "print(\"User representation shape:\", user_representation.shape)  # Expected: (16, 300)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)  # Expected: (16, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines MultiHeadAdditiveAttention and UserAdditiveAttention\n",
    "    to encode user representations based on news embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - embed_dim: Dimensionality of the input embeddings (news representations).\n",
    "        - num_heads: Number of attention heads in the MultiHeadAdditiveAttention layer.\n",
    "        \"\"\"\n",
    "        super(UserEncoder, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAdditiveAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.user_attention = UserAdditiveAttention(embed_dim=embed_dim)\n",
    "\n",
    "    def forward(self, news_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - news_embeddings: Tensor of shape (batch_size, num_news, embed_dim),\n",
    "                           representing the news representations.\n",
    "\n",
    "        Returns:\n",
    "        - user_representation: Tensor of shape (batch_size, embed_dim),\n",
    "                               the final user representation.\n",
    "        - attention_weights: Dictionary containing:\n",
    "            - 'multi_head_attention': List of tensors of shape (batch_size, num_news, num_news) per head,\n",
    "                                      attention weights for each head from the MultiHeadAdditiveAttention layer.\n",
    "            - 'user_attention': Tensor of shape (batch_size, num_news),\n",
    "                                attention weights for the news articles from the UserAdditiveAttention layer.\n",
    "        \"\"\"\n",
    "        enhanced_news_embeddings, multi_head_attention_weights = self.multi_head_attention(news_embeddings)\n",
    "        user_representation, user_attention_weights = self.user_attention(enhanced_news_embeddings)\n",
    "\n",
    "        attention_weights = {\n",
    "            'multi_head_attention': multi_head_attention_weights,\n",
    "            'user_attention': user_attention_weights\n",
    "        }\n",
    "        return user_representation, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define parameters\n",
    "embed_dim = 300    # Embedding dimension\n",
    "num_heads = 10     # Number of attention heads\n",
    "batch_size = 4     # Number of users in the batch\n",
    "num_articles = 5   # Number of articles browsed by each user\n",
    "\n",
    "# Simulate input data (news representations from News Encoder)\n",
    "user_input = torch.randn(batch_size, num_articles, embed_dim)  # Random tensor for simulation\n",
    "\n",
    "# Initialize the UserEncoder\n",
    "user_encoder = UserEncoder(embed_dim=embed_dim, num_heads=num_heads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User representation shape: torch.Size([16, 300])\n",
      "Multi-head attention weights shape (head 0): torch.Size([16, 10, 10])\n",
      "User attention weights shape: torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 16\n",
    "num_news = 10  # Number of news articles browsed by each user\n",
    "embed_dim = 300\n",
    "num_heads = 10\n",
    "\n",
    "# Dummy input\n",
    "news_embeddings = torch.rand(batch_size, num_news, embed_dim)  # Random news embeddings\n",
    "\n",
    "# Initialize the UserEncoder\n",
    "user_encoder = UserEncoder(embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "# Forward pass\n",
    "user_representation, attention_weights = user_encoder(news_embeddings)\n",
    "\n",
    "# Output shapes\n",
    "print(\"User representation shape:\", user_representation.shape)  # Expected: (16, 300)\n",
    "print(\"Multi-head attention weights shape (head 0):\", attention_weights['multi_head_attention'][0].shape)  # Expected: (16, 10, 10)\n",
    "print(\"User attention weights shape:\", attention_weights['user_attention'].shape)  # Expected: (16, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Execution of every aspect of the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_user_encoder_initialization(user_encoder):\n",
    "    assert user_encoder is not None, \"UserEncoder initialization failed.\"\n",
    "    print(\"UserEncoder initialization test passed.\")\n",
    "\n",
    "def test_user_encoder_forward(user_encoder, news_embeddings):\n",
    "    user_representation, attention_weights = user_encoder(news_embeddings)\n",
    "    assert user_representation.shape == (16, 64), \"User representation shape mismatch!\"\n",
    "    assert 'multi_head_attention' in attention_weights, \"Missing multi_head_attention key!\"\n",
    "    assert 'user_attention' in attention_weights, \"Missing user_attention key!\"\n",
    "    print(\"UserEncoder forward pass test passed.\")\n",
    "\n",
    "def test_multi_head_attention(multi_head_attention, news_embeddings):\n",
    "    enhanced_news_embeddings, attention_weights = multi_head_attention(news_embeddings)\n",
    "    assert enhanced_news_embeddings.shape == (16, 10, 64), \"Multi-head attention output shape mismatch!\"\n",
    "    print(\"MultiHeadAdditiveAttention test passed.\")\n",
    "\n",
    "def test_user_additive_attention(user_attention, news_embeddings):\n",
    "    user_representation, attention_weights = user_attention(news_embeddings)\n",
    "    assert user_representation.shape == (16, 64), \"User representation shape mismatch!\"\n",
    "    assert attention_weights.shape == (16, 10), \"Attention weights shape mismatch!\"\n",
    "    print(\"UserAdditiveAttention test passed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News embeddings initialized: torch.Size([16, 10, 64])\n",
      "UserEncoder initialized.\n",
      "UserEncoder initialization test passed.\n",
      "UserEncoder forward pass test passed.\n",
      "MultiHeadAdditiveAttention test passed.\n",
      "UserAdditiveAttention test passed.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize test inputs\n",
    "    batch_size = 16\n",
    "    num_news = 10\n",
    "    embed_dim = 64\n",
    "    num_heads = 4\n",
    "\n",
    "    # Create random news embeddings\n",
    "    news_embeddings = torch.randn(batch_size, num_news, embed_dim)\n",
    "    print(\"News embeddings initialized:\", news_embeddings.shape)\n",
    "\n",
    "    # Initialize UserEncoder\n",
    "    user_encoder = UserEncoder(embed_dim=embed_dim, num_heads=num_heads)\n",
    "    print(\"UserEncoder initialized.\")\n",
    "\n",
    "    # Test: Initialization\n",
    "    test_user_encoder_initialization(user_encoder)\n",
    "\n",
    "    # Test: Forward pass\n",
    "    test_user_encoder_forward(user_encoder, news_embeddings)\n",
    "\n",
    "    # Test: MultiHeadAdditiveAttention\n",
    "    multi_head_attention = MultiHeadAdditiveAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "    test_multi_head_attention(multi_head_attention, news_embeddings)\n",
    "\n",
    "    # Test: UserAdditiveAttention\n",
    "    user_attention = UserAdditiveAttention(embed_dim=embed_dim)\n",
    "    test_user_additive_attention(user_attention, news_embeddings)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input Shape: torch.Size([4, 5, 300])\n",
      "UserEncoder initialized successfully.\n",
      "Forward pass completed successfully.\n",
      "User Representation Shape: torch.Size([4, 300])\n",
      "Error validating output shapes: 'dict' object has no attribute 'shape'\n",
      "Error inspecting attention weights: 0\n"
     ]
    }
   ],
   "source": [
    "# Debugging Script\n",
    "\n",
    "import torch\n",
    "\n",
    "# Define Parameters\n",
    "batch_size = 4    # Number of users in the batch\n",
    "num_articles = 5  # Number of articles browsed per user\n",
    "embed_dim = 300   # Embedding dimension (output from News Encoder)\n",
    "num_heads = 10    # Number of attention heads\n",
    "\n",
    "# Generate Input Data\n",
    "user_input = torch.randn(batch_size, num_articles, embed_dim)\n",
    "print(\"User Input Shape:\", user_input.shape)\n",
    "\n",
    "# Initialize the User Encoder\n",
    "try:\n",
    "    user_encoder = UserEncoder(embed_dim=embed_dim, num_heads=num_heads)\n",
    "    print(\"UserEncoder initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during UserEncoder initialization: {e}\")\n",
    "\n",
    "# Forward Pass\n",
    "try:\n",
    "    user_representation, attention_weights = user_encoder(user_input)\n",
    "    print(\"Forward pass completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during forward pass: {e}\")\n",
    "\n",
    "# Validate Output Shapes\n",
    "try:\n",
    "    print(\"User Representation Shape:\", user_representation.shape)  # Expected: (batch_size, embed_dim)\n",
    "    print(\"Attention Weights Shape:\", attention_weights.shape)      # Expected: (batch_size, num_articles)\n",
    "except Exception as e:\n",
    "    print(f\"Error validating output shapes: {e}\")\n",
    "\n",
    "# Inspect Attention Weights for the First User\n",
    "try:\n",
    "    print(\"\\nAttention Weights for User 1:\\n\", attention_weights[0].detach().numpy())  # Shape: (num_articles)\n",
    "except Exception as e:\n",
    "    print(f\"Error inspecting attention weights: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
