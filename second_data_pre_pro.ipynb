{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demo and small datasets provided by EB-Nerd are subsets of the full dataset, designed for different levels of experimentation and prototypig. \n",
    "\n",
    "\n",
    "- We will use the demo dataset for the beggining in order to develop our model. and to quickly validate our core or preprocessing pipeline. \n",
    "\n",
    "- Then we will use the small dataset to verify that our code works the demo dataset, because it is a more representative subset for training an evaluating the models . \n",
    "\n",
    "- The large dataset, requires significant computational resources, and it is more time consuming to process so we should used it only after confirming the pipeline works correctly with smaller datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's sarts with all the essential preprocessing steps required for the NRMS model and incorporates the data cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Was Included\n",
    "1. Load and Merge Data:\n",
    "\n",
    "Combined history.parquet and behaviors.parquet for both training and validation datasets.\n",
    "Ensured the userâ€™s click history (clicked_articles) and behaviors (inview_articles) were merged properly.\n",
    "\n",
    "2. Explode inview_articles:\n",
    "\n",
    "Expanded inview_articles into individual rows.\n",
    "Generated binary labels (1 for clicked articles, 0 for others).\n",
    "\n",
    "3. Tokenize and Encode Articles:\n",
    "\n",
    "Used XLM-RoBERTa to tokenize and generate embeddings for news titles and subtitles.\n",
    "Prepared these embeddings for use by the NRMS model.\n",
    "\n",
    "4. Create Article Mappings:\n",
    "\n",
    "Mapped article IDs to their tokenized/encoded representations for efficient lookup.\n",
    "\n",
    "5. Negative Sampling:\n",
    "\n",
    "Paired each positive (clicked) article with multiple negative (non-clicked) samples.\n",
    "Ensured balanced data for training the NRMS model.\n",
    "\n",
    "6. Prepare Dataloaders:\n",
    "\n",
    "Formatted the data into batches using NRMSDataLoaderPretransform.\n",
    "This ensures compatibility with the NRMS training process.\n",
    "\n",
    "7. Data Cleaning (from your notebook):\n",
    "\n",
    "Dropped unnecessary columns (impression_time, articles_num, etc.) from the dataset.\n",
    "Verified column existence before dropping to avoid errors.\n",
    "Ensured the dataset is clean and optimized for processing.\n",
    "\n",
    "8. Efficient Output:\n",
    "\n",
    "Saved the final processed datasets in chunked Parquet files, allowing efficient handling of large files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants defined in our code represent hey parameters, column names and configuration values used throughout the preprocessing and model pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Define constants\n",
    "\n",
    "#Dataset columns names\n",
    "DEFAULT_USER_COL = \"user_id\"\n",
    "DEFAULT_HISTORY_ARTICLE_ID_COL = \"article_id_fixed\"\n",
    "DEFAULT_CLICKED_ARTICLES_COL = \"article_ids_clicked\"\n",
    "DEFAULT_INVIEW_ARTICLES_COL = \"article_ids_inview\"\n",
    "DEFAULT_TITLE_COL = \"title\"\n",
    "DEFAULT_SUBTITLE_COL = \"subtitle\"\n",
    "DEFAULT_LABELS_COL = \"label\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Transformer model name \n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "\n",
    "# Preprocessing Parameters \n",
    "HISTORY_SIZE = 20\n",
    "MAX_TITLE_LENGTH = 30\n",
    "NPRATIO = 4\n",
    "SEED = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Defines file paths\n",
    "2. Defines an output directory to save processed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are found!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the base dataset path relative to your current location\n",
    "DATASET_PATH = Path(\"c:/Users/Lydia/OneDrive - Danmarks Tekniske Universitet/DTU master , lectures and exercises/Deep learning my exercises/Deep-learning_final_project-2/ebnerd_demo\")\n",
    "\n",
    "# Define file paths\n",
    "HISTORY_PATH = DATASET_PATH / \"train/history.parquet\"  # history.parquet in train folder\n",
    "TRAIN_BEHAVIORS_PATH = DATASET_PATH / \"train/behaviors.parquet\"  # behaviors.parquet in train folder\n",
    "ARTICLES_PATH = DATASET_PATH / \"articles.parquet\"  # articles.parquet directly in ebnerd_demo\n",
    "\n",
    "# Verify that the files exist\n",
    "def verify_file_paths(*paths):\n",
    "    for path in paths:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "verify_file_paths(HISTORY_PATH, TRAIN_BEHAVIORS_PATH, ARTICLES_PATH)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"All files are found!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(Path(HISTORY_PATH).exists())  # Should return True if the file exists\n",
    "print(Path(TRAIN_BEHAVIORS_PATH).exists())  # Should return True\n",
    "print(Path(ARTICLES_PATH).exists())  # Should return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the script we are performing the data clueaning and merging step. \n",
    "1. Loading the datasets\n",
    "2. Truncating click history: Limits click history to a fixed size for consistency.\n",
    "The above means limiting the number of articles in a user's click history to a fixed size. For example for users with long click histories , we date only a number of them (5), and if a user has a short click histories, the history is padded with zeros to match the required size. All click histories are the same length.\n",
    "\n",
    "\n",
    "\n",
    "3. Merging datasets : combines user click history with their behavior logs\n",
    "4. Cleaning the data: Drops irrelevant columns to reduce dataset size and focus on required information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to drop (from your notebook)\n",
    "COLUMNS_TO_DROP = [\n",
    "    \"impression_time\", \"articles_num\", \"last_modified_time\", \"body\",\n",
    "    \"published_time_x\", \"published_time_y\", \"subcategory\", \"ner_clusters\",\n",
    "    \"entity_groups\", \"impr_pub_hour\"\n",
    "]\n",
    "\n",
    "# Define a custom truncate_history function\n",
    "def truncate_history(column, history_size, padding_value=0):\n",
    "    \"\"\"\n",
    "    Truncates or pads a list of clicked articles to a fixed history size.\n",
    "    \"\"\"\n",
    "    return column.arr.slice(0, history_size).arr.eval(\n",
    "        lambda x: x + [padding_value] * (history_size - len(x)) if len(x) < history_size else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Updated load_and_clean_data function\n",
    "def load_and_clean_data(history_path, behaviors_path, articles_path):\n",
    "    # Load history.parquet\n",
    "    df_history = pl.read_parquet(history_path)\n",
    "\n",
    "    # Group by user_id and aggregate articles into a list\n",
    "    df_history_grouped = df_history.groupby(DEFAULT_USER_COL).agg(\n",
    "        pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).list().alias(DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "    )\n",
    "\n",
    "    # Truncate the article history\n",
    "    df_history_grouped = df_history_grouped.with_columns(\n",
    "        truncate_history(pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL), HISTORY_SIZE, 0).alias(DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "    )\n",
    "\n",
    "    # Load behaviors.parquet and join with grouped history\n",
    "    df_behaviors = pl.read_parquet(behaviors_path).join(\n",
    "        df_history_grouped,\n",
    "        on=DEFAULT_USER_COL,\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    columns_to_drop = [col for col in COLUMNS_TO_DROP if col in df_behaviors.columns]\n",
    "    df_behaviors = df_behaviors.drop(columns=columns_to_drop)\n",
    "\n",
    "    return df_behaviors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are preparing the inview articles column for the next steps in the preprocessing pipeline.\n",
    "\n",
    "1. Converts the list of inview_articles into individual rows, with each row corresponding to a single article, because the nrms model works on a per-article basis, comparing each article the user saw against their click history to predict clicks. Exploding ensures each article in the list is processed individually. \n",
    "\n",
    "\n",
    "\n",
    "Example: Before Exploding:\n",
    "\n",
    "- user_id\tinview_articles\n",
    "\n",
    "1\t[101, 102, 103]\n",
    "\n",
    "2\t[201, 202]\n",
    "\n",
    "After Exploding:\n",
    "\n",
    "- user_id\tinview_article\n",
    "\n",
    "1\t101\n",
    "\n",
    "1\t102\n",
    "\n",
    "1\t103\n",
    "\n",
    "2\t201\n",
    "\n",
    "2\t202\n",
    "\n",
    "\n",
    "\n",
    "2. Assigns a binary label (1 or 0) to each inview_article, indicating whether the user clicked on the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode and add labels\n",
    "def explode_inview_articles(df):\n",
    "    return (\n",
    "        df.with_columns(\n",
    "            pl.col(DEFAULT_INVIEW_ARTICLES_COL).list.explode().alias(\"inview_article\")\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col(\"inview_article\").is_in(pl.col(DEFAULT_CLICKED_ARTICLES_COL)))\n",
    "            .then(1)\n",
    "            .otherwise(0)\n",
    "            .alias(DEFAULT_LABELS_COL)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is responsible for preparing the textual content in our dataset by tokenizing and encoding it into numerical embeddings using a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize articles\n",
    "def tokenize_and_encode_articles(articles_path):\n",
    "    df_articles = pl.read_parquet(articles_path)\n",
    "\n",
    "    # Load transformer model and tokenizer\n",
    "    transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "    transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "    # Tokenize and encode text\n",
    "    df_articles = convert_text2encoding_with_transformers(\n",
    "        df_articles, transformer_tokenizer, DEFAULT_TITLE_COL, max_length=MAX_TITLE_LENGTH\n",
    "    )\n",
    "\n",
    "    return df_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sampling\n",
    "def apply_negative_sampling(df):\n",
    "    return df.pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=NPRATIO,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'groupby'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal dataset saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Main preprocessing pipeline\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m df_train_behaviors \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_clean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHISTORY_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_BEHAVIORS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mARTICLES_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m df_train_behaviors \u001b[38;5;241m=\u001b[39m explode_inview_articles(df_train_behaviors)\n\u001b[0;32m     21\u001b[0m df_train_behaviors \u001b[38;5;241m=\u001b[39m apply_negative_sampling(df_train_behaviors)\n",
      "Cell \u001b[1;32mIn[64], line 25\u001b[0m, in \u001b[0;36mload_and_clean_data\u001b[1;34m(history_path, behaviors_path, articles_path)\u001b[0m\n\u001b[0;32m     22\u001b[0m df_history \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mread_parquet(history_path)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Group by user_id and aggregate articles into a list\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m df_history_grouped \u001b[38;5;241m=\u001b[39m \u001b[43mdf_history\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m(DEFAULT_USER_COL)\u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m     26\u001b[0m     pl\u001b[38;5;241m.\u001b[39mcol(DEFAULT_HISTORY_ARTICLE_ID_COL)\u001b[38;5;241m.\u001b[39mlist()\u001b[38;5;241m.\u001b[39malias(DEFAULT_HISTORY_ARTICLE_ID_COL)\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Truncate the article history\u001b[39;00m\n\u001b[0;32m     30\u001b[0m df_history_grouped \u001b[38;5;241m=\u001b[39m df_history_grouped\u001b[38;5;241m.\u001b[39mwith_columns(\n\u001b[0;32m     31\u001b[0m     truncate_history(pl\u001b[38;5;241m.\u001b[39mcol(DEFAULT_HISTORY_ARTICLE_ID_COL), HISTORY_SIZE, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39malias(DEFAULT_HISTORY_ARTICLE_ID_COL)\n\u001b[0;32m     32\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'groupby'"
     ]
    }
   ],
   "source": [
    "# Save large datasets in chunks\n",
    "def save_dataset_in_chunks(df, output_file, chunk_size=1_000_000):\n",
    "    table = pa.Table.from_pandas(df.iloc[:1])  # Use the first row to get schema\n",
    "    writer = pq.ParquetWriter(output_file, table.schema, compression=\"gzip\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    print(f\"Starting to write {total_rows} rows in chunks...\")\n",
    "\n",
    "    for start in range(0, total_rows, chunk_size):\n",
    "        chunk = df.iloc[start:start + chunk_size]\n",
    "        table = pa.Table.from_pandas(chunk)\n",
    "        writer.write_table(table)\n",
    "        print(f\"Written rows {start} to {start + len(chunk) - 1}.\")\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Final dataset saved as '{output_file}'\")\n",
    "\n",
    "# Main preprocessing pipeline\n",
    "df_train_behaviors = load_and_clean_data(HISTORY_PATH, TRAIN_BEHAVIORS_PATH, ARTICLES_PATH)\n",
    "df_train_behaviors = explode_inview_articles(df_train_behaviors)\n",
    "df_train_behaviors = apply_negative_sampling(df_train_behaviors)\n",
    "save_dataset_in_chunks(df_train_behaviors, OUTPUT_PATH / \"train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are found!\n",
      "Starting to write 278139 rows in chunks...\n",
      "Written rows 0 to 278138.\n",
      "Final dataset saved as 'c:\\Users\\Lydia\\OneDrive - Danmarks Tekniske Universitet\\DTU master , lectures and exercises\\Deep learning my exercises\\Deep-learning_final_project-2\\ebnerd_demo\\processed\\train.parquet'\n",
      "Data preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Define constants\n",
    "\n",
    "# Dataset columns names\n",
    "DEFAULT_USER_COL = \"user_id\"\n",
    "DEFAULT_HISTORY_ARTICLE_ID_COL = \"article_id_fixed\"\n",
    "DEFAULT_CLICKED_ARTICLES_COL = \"article_ids_clicked\"\n",
    "DEFAULT_INVIEW_ARTICLES_COL = \"article_ids_inview\"\n",
    "DEFAULT_TITLE_COL = \"title\"\n",
    "DEFAULT_SUBTITLE_COL = \"subtitle\"\n",
    "DEFAULT_LABELS_COL = \"label\"\n",
    "\n",
    "# Transformer model name\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "\n",
    "# Preprocessing Parameters\n",
    "HISTORY_SIZE = 20\n",
    "MAX_TITLE_LENGTH = 30\n",
    "NPRATIO = 4\n",
    "SEED = 42\n",
    "\n",
    "# Define the base dataset path relative to your current location\n",
    "DATASET_PATH = Path(\n",
    "    \"c:/Users/Lydia/OneDrive - Danmarks Tekniske Universitet/DTU master , lectures and exercises/Deep learning my exercises/Deep-learning_final_project-2/ebnerd_demo\"\n",
    ")\n",
    "\n",
    "# Define file paths\n",
    "HISTORY_PATH = DATASET_PATH / \"train/history.parquet\"  # history.parquet in train folder\n",
    "TRAIN_BEHAVIORS_PATH = DATASET_PATH / \"train/behaviors.parquet\"  # behaviors.parquet in train folder\n",
    "ARTICLES_PATH = DATASET_PATH / \"articles.parquet\"  # articles.parquet directly in ebnerd_demo\n",
    "\n",
    "# Verify that the files exist\n",
    "def verify_file_paths(*paths):\n",
    "    for path in paths:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "verify_file_paths(HISTORY_PATH, TRAIN_BEHAVIORS_PATH, ARTICLES_PATH)\n",
    "print(\"All files are found!\")\n",
    "\n",
    "# Define columns to drop\n",
    "COLUMNS_TO_DROP = [\n",
    "    \"impression_time\", \"scroll_percentage\", \"device_type\", \"session_id\",\n",
    "    \"next_read_time\", \"next_scroll_percentage\", \"postcode\", \"age\",\n",
    "    \"is_subscriber\", \"gender\", \"is_sso_user\"\n",
    "]\n",
    "\n",
    "# Define a custom truncate_history function\n",
    "def truncate_history_manual(history, history_size, padding_value=0):\n",
    "    \"\"\"\n",
    "    Manually truncate or pad a list of article histories.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        h[:history_size] + [padding_value] * max(0, history_size - len(h))\n",
    "        for h in history\n",
    "    ]\n",
    "\n",
    "# Load and clean data\n",
    "def load_and_clean_data(history_path, behaviors_path):\n",
    "    # Load history.parquet\n",
    "    df_history = pd.read_parquet(history_path)\n",
    "\n",
    "    # Verify required columns\n",
    "    required_columns = [DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL]\n",
    "    for col in required_columns:\n",
    "        if col not in df_history.columns:\n",
    "            raise ValueError(f\"Missing required column: {col} in history.parquet\")\n",
    "\n",
    "    # Aggregate articles into lists per user\n",
    "    user_history = df_history.groupby(DEFAULT_USER_COL)[DEFAULT_HISTORY_ARTICLE_ID_COL].apply(list).reset_index()\n",
    "\n",
    "    # Truncate histories\n",
    "    user_history[DEFAULT_HISTORY_ARTICLE_ID_COL] = truncate_history_manual(\n",
    "        user_history[DEFAULT_HISTORY_ARTICLE_ID_COL], HISTORY_SIZE, 0\n",
    "    )\n",
    "\n",
    "    # Load behaviors.parquet\n",
    "    df_behaviors = pd.read_parquet(behaviors_path)\n",
    "\n",
    "    # Verify required columns\n",
    "    required_behavior_columns = [\n",
    "        DEFAULT_USER_COL,\n",
    "        DEFAULT_CLICKED_ARTICLES_COL,\n",
    "        DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    ]\n",
    "    for col in required_behavior_columns:\n",
    "        if col not in df_behaviors.columns:\n",
    "            raise ValueError(f\"Missing required column: {col} in behaviors.parquet\")\n",
    "\n",
    "    # Merge behaviors with truncated history\n",
    "    df_behaviors = pd.merge(df_behaviors, user_history, on=DEFAULT_USER_COL, how=\"left\")\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    df_behaviors.drop(columns=[col for col in COLUMNS_TO_DROP if col in df_behaviors.columns], inplace=True)\n",
    "\n",
    "    return df_behaviors\n",
    "\n",
    "# Explode and add labels\n",
    "def explode_inview_articles(df):\n",
    "    \"\"\"\n",
    "    Explode the inview articles and add labels.\n",
    "    \"\"\"\n",
    "    df_exploded = df.explode(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "    df_exploded[DEFAULT_LABELS_COL] = df_exploded.apply(\n",
    "        lambda row: 1 if row[DEFAULT_INVIEW_ARTICLES_COL] in row[DEFAULT_CLICKED_ARTICLES_COL] else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    return df_exploded\n",
    "\n",
    "# Tokenize articles\n",
    "def tokenize_and_encode_articles(articles_path):\n",
    "    df_articles = pd.read_parquet(articles_path)\n",
    "\n",
    "    # Verify required columns\n",
    "    required_article_columns = [DEFAULT_TITLE_COL, DEFAULT_SUBTITLE_COL]\n",
    "    for col in required_article_columns:\n",
    "        if col not in df_articles.columns:\n",
    "            raise ValueError(f\"Missing required column: {col} in articles.parquet\")\n",
    "\n",
    "    # Load transformer model and tokenizer\n",
    "    transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "    transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "    # Tokenize and encode text\n",
    "    # Add your text tokenization logic here, e.g., apply transformer tokenizer on titles\n",
    "\n",
    "    return df_articles\n",
    "\n",
    "# Negative sampling\n",
    "def apply_negative_sampling(df):\n",
    "    \"\"\"\n",
    "    Apply negative sampling to the dataset.\n",
    "    \"\"\"\n",
    "    # Implement your negative sampling logic here\n",
    "    pass\n",
    "\n",
    "# Save large datasets in chunks\n",
    "def save_dataset_in_chunks(df, output_file, chunk_size=1_000_000):\n",
    "    \"\"\"\n",
    "    Save large datasets in chunks using PyArrow Parquet.\n",
    "    \"\"\"\n",
    "    # Ensure all list columns are represented consistently as strings for saving\n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            df[col] = df[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "            df[col] = df[col].apply(str)  # Convert lists to strings\n",
    "    \n",
    "    # Convert to PyArrow table\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    writer = pq.ParquetWriter(output_file, table.schema, compression=\"gzip\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    print(f\"Starting to write {total_rows} rows in chunks...\")\n",
    "\n",
    "    for start in range(0, total_rows, chunk_size):\n",
    "        chunk = df.iloc[start:start + chunk_size]\n",
    "        table = pa.Table.from_pandas(chunk)\n",
    "        writer.write_table(table)\n",
    "        print(f\"Written rows {start} to {start + len(chunk) - 1}.\")\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Final dataset saved as '{output_file}'\")\n",
    "\n",
    "\n",
    "# Main preprocessing pipeline\n",
    "OUTPUT_PATH = DATASET_PATH / \"processed\"\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_train_behaviors = load_and_clean_data(HISTORY_PATH, TRAIN_BEHAVIORS_PATH)\n",
    "df_train_behaviors = explode_inview_articles(df_train_behaviors)\n",
    "save_dataset_in_chunks(df_train_behaviors, OUTPUT_PATH / \"train.parquet\")\n",
    "\n",
    "print(\"Data preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in history.parquet: ['user_id', 'impression_time_fixed', 'scroll_percentage_fixed', 'article_id_fixed', 'read_time_fixed']\n",
      "Columns in behaviors.parquet: ['impression_id', 'article_id', 'impression_time', 'read_time', 'scroll_percentage', 'device_type', 'article_ids_inview', 'article_ids_clicked', 'user_id', 'is_sso_user', 'gender', 'postcode', 'age', 'is_subscriber', 'session_id', 'next_read_time', 'next_scroll_percentage']\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Check columns in history.parquet\n",
    "history_df = pl.read_parquet(HISTORY_PATH)\n",
    "print(\"Columns in history.parquet:\", history_df.columns)\n",
    "\n",
    "# Check columns in behaviors.parquet\n",
    "behaviors_df = pl.read_parquet(TRAIN_BEHAVIORS_PATH)\n",
    "print(\"Columns in behaviors.parquet:\", behaviors_df.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>read_time</th>\n",
       "      <th>article_ids_inview</th>\n",
       "      <th>article_ids_clicked</th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id_fixed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9774516</td>\n",
       "      <td>[9759966]</td>\n",
       "      <td>22779</td>\n",
       "      <td>[array([9738452, 9737521, 9738760, 9733713, 97...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9771051</td>\n",
       "      <td>[9759966]</td>\n",
       "      <td>22779</td>\n",
       "      <td>[array([9738452, 9737521, 9738760, 9733713, 97...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9770028</td>\n",
       "      <td>[9759966]</td>\n",
       "      <td>22779</td>\n",
       "      <td>[array([9738452, 9737521, 9738760, 9733713, 97...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9775402</td>\n",
       "      <td>[9759966]</td>\n",
       "      <td>22779</td>\n",
       "      <td>[array([9738452, 9737521, 9738760, 9733713, 97...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9774461</td>\n",
       "      <td>[9759966]</td>\n",
       "      <td>22779</td>\n",
       "      <td>[array([9738452, 9737521, 9738760, 9733713, 97...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  article_id  read_time  article_ids_inview  \\\n",
       "0          48401         NaN       21.0             9774516   \n",
       "0          48401         NaN       21.0             9771051   \n",
       "0          48401         NaN       21.0             9770028   \n",
       "0          48401         NaN       21.0             9775402   \n",
       "0          48401         NaN       21.0             9774461   \n",
       "\n",
       "  article_ids_clicked  user_id  \\\n",
       "0           [9759966]    22779   \n",
       "0           [9759966]    22779   \n",
       "0           [9759966]    22779   \n",
       "0           [9759966]    22779   \n",
       "0           [9759966]    22779   \n",
       "\n",
       "                                    article_id_fixed  label  \n",
       "0  [array([9738452, 9737521, 9738760, 9733713, 97...      0  \n",
       "0  [array([9738452, 9737521, 9738760, 9733713, 97...      0  \n",
       "0  [array([9738452, 9737521, 9738760, 9733713, 97...      0  \n",
       "0  [array([9738452, 9737521, 9738760, 9733713, 97...      0  \n",
       "0  [array([9738452, 9737521, 9738760, 9733713, 97...      0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your train.parquet file\n",
    "train_parquet_path = \"c:/Users/Lydia/OneDrive - Danmarks Tekniske Universitet/DTU master , lectures and exercises/Deep learning my exercises/Deep-learning_final_project-2/ebnerd_demo/processed/train.parquet\"\n",
    "\n",
    "# Read the Parquet file\n",
    "df_train = pd.read_parquet(train_parquet_path)\n",
    "\n",
    "# Print the first few rows\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'History Columns': ['user_id',\n",
       "  'impression_time_fixed',\n",
       "  'scroll_percentage_fixed',\n",
       "  'article_id_fixed',\n",
       "  'read_time_fixed'],\n",
       " 'Behaviors Columns': ['impression_id',\n",
       "  'article_id',\n",
       "  'impression_time',\n",
       "  'read_time',\n",
       "  'scroll_percentage',\n",
       "  'device_type',\n",
       "  'article_ids_inview',\n",
       "  'article_ids_clicked',\n",
       "  'user_id',\n",
       "  'is_sso_user',\n",
       "  'gender',\n",
       "  'postcode',\n",
       "  'age',\n",
       "  'is_subscriber',\n",
       "  'session_id',\n",
       "  'next_read_time',\n",
       "  'next_scroll_percentage'],\n",
       " 'Articles Columns': ['article_id',\n",
       "  'title',\n",
       "  'subtitle',\n",
       "  'last_modified_time',\n",
       "  'premium',\n",
       "  'body',\n",
       "  'published_time',\n",
       "  'image_ids',\n",
       "  'article_type',\n",
       "  'url',\n",
       "  'ner_clusters',\n",
       "  'entity_groups',\n",
       "  'topics',\n",
       "  'category',\n",
       "  'subcategory',\n",
       "  'category_str',\n",
       "  'total_inviews',\n",
       "  'total_pageviews',\n",
       "  'total_read_time',\n",
       "  'sentiment_score',\n",
       "  'sentiment_label']}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the paths for the datasets\n",
    "history_path = \"c:/Users/Lydia/OneDrive - Danmarks Tekniske Universitet/DTU master , lectures and exercises/Deep learning my exercises/Deep-learning_final_project-2/ebnerd_demo/train/history.parquet\"\n",
    "behaviors_path = \"c:/Users/Lydia/OneDrive - Danmarks Tekniske Universitet/DTU master , lectures and exercises/Deep learning my exercises/Deep-learning_final_project-2/ebnerd_demo/train/behaviors.parquet\"\n",
    "articles_path = \"c:/Users/Lydia/OneDrive - Danmarks Tekniske Universitet/DTU master , lectures and exercises/Deep learning my exercises/Deep-learning_final_project-2/ebnerd_demo/articles.parquet\"\n",
    "\n",
    "# Read the datasets\n",
    "history_df = pd.read_parquet(history_path)\n",
    "behaviors_df = pd.read_parquet(behaviors_path)\n",
    "articles_df = pd.read_parquet(articles_path)\n",
    "\n",
    "# Print the columns of each dataset\n",
    "{\n",
    "    \"History Columns\": history_df.columns.tolist(),\n",
    "    \"Behaviors Columns\": behaviors_df.columns.tolist(),\n",
    "    \"Articles Columns\": articles_df.columns.tolist()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>impression_time_fixed</th>\n",
       "      <th>scroll_percentage_fixed</th>\n",
       "      <th>article_id_fixed</th>\n",
       "      <th>read_time_fixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13538</td>\n",
       "      <td>[2023-04-27T10:17:43.000000, 2023-04-27T10:18:...</td>\n",
       "      <td>[100.0, 35.0, 100.0, 24.0, 100.0, 23.0, 100.0,...</td>\n",
       "      <td>[9738663, 9738569, 9738663, 9738490, 9738663, ...</td>\n",
       "      <td>[17.0, 12.0, 4.0, 5.0, 4.0, 9.0, 5.0, 46.0, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58608</td>\n",
       "      <td>[2023-04-27T18:48:09.000000, 2023-04-27T18:48:...</td>\n",
       "      <td>[37.0, 61.0, 100.0, 100.0, 55.0, 100.0, 100.0,...</td>\n",
       "      <td>[9739362, 9739179, 9738567, 9739344, 9739202, ...</td>\n",
       "      <td>[2.0, 24.0, 72.0, 65.0, 11.0, 4.0, 101.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95507</td>\n",
       "      <td>[2023-04-27T15:20:28.000000, 2023-04-27T15:20:...</td>\n",
       "      <td>[60.0, 100.0, 100.0, 21.0, 29.0, 67.0, 49.0, 5...</td>\n",
       "      <td>[9739035, 9738646, 9634967, 9738902, 9735495, ...</td>\n",
       "      <td>[18.0, 29.0, 51.0, 12.0, 10.0, 10.0, 13.0, 24....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106588</td>\n",
       "      <td>[2023-04-27T08:29:09.000000, 2023-04-27T08:29:...</td>\n",
       "      <td>[24.0, 57.0, 100.0, nan, nan, 100.0, 100.0, 73...</td>\n",
       "      <td>[9738292, 9738216, 9737266, 9737556, 9737657, ...</td>\n",
       "      <td>[9.0, 15.0, 42.0, 9.0, 3.0, 58.0, 26.0, 214.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>617963</td>\n",
       "      <td>[2023-04-27T14:42:25.000000, 2023-04-27T14:43:...</td>\n",
       "      <td>[100.0, 100.0, nan, 46.0, 23.0, 19.0, 61.0, 70...</td>\n",
       "      <td>[9739035, 9739088, 9738902, 9738968, 9738760, ...</td>\n",
       "      <td>[45.0, 29.0, 116.0, 26.0, 34.0, 42.0, 58.0, 59...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                              impression_time_fixed  \\\n",
       "0    13538  [2023-04-27T10:17:43.000000, 2023-04-27T10:18:...   \n",
       "1    58608  [2023-04-27T18:48:09.000000, 2023-04-27T18:48:...   \n",
       "2    95507  [2023-04-27T15:20:28.000000, 2023-04-27T15:20:...   \n",
       "3   106588  [2023-04-27T08:29:09.000000, 2023-04-27T08:29:...   \n",
       "4   617963  [2023-04-27T14:42:25.000000, 2023-04-27T14:43:...   \n",
       "\n",
       "                             scroll_percentage_fixed  \\\n",
       "0  [100.0, 35.0, 100.0, 24.0, 100.0, 23.0, 100.0,...   \n",
       "1  [37.0, 61.0, 100.0, 100.0, 55.0, 100.0, 100.0,...   \n",
       "2  [60.0, 100.0, 100.0, 21.0, 29.0, 67.0, 49.0, 5...   \n",
       "3  [24.0, 57.0, 100.0, nan, nan, 100.0, 100.0, 73...   \n",
       "4  [100.0, 100.0, nan, 46.0, 23.0, 19.0, 61.0, 70...   \n",
       "\n",
       "                                    article_id_fixed  \\\n",
       "0  [9738663, 9738569, 9738663, 9738490, 9738663, ...   \n",
       "1  [9739362, 9739179, 9738567, 9739344, 9739202, ...   \n",
       "2  [9739035, 9738646, 9634967, 9738902, 9735495, ...   \n",
       "3  [9738292, 9738216, 9737266, 9737556, 9737657, ...   \n",
       "4  [9739035, 9739088, 9738902, 9738968, 9738760, ...   \n",
       "\n",
       "                                     read_time_fixed  \n",
       "0  [17.0, 12.0, 4.0, 5.0, 4.0, 9.0, 5.0, 46.0, 11...  \n",
       "1  [2.0, 24.0, 72.0, 65.0, 11.0, 4.0, 101.0, 0.0,...  \n",
       "2  [18.0, 29.0, 51.0, 12.0, 10.0, 10.0, 13.0, 24....  \n",
       "3  [9.0, 15.0, 42.0, 9.0, 3.0, 58.0, 26.0, 214.0,...  \n",
       "4  [45.0, 29.0, 116.0, 26.0, 34.0, 42.0, 58.0, 59...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>impression_time</th>\n",
       "      <th>read_time</th>\n",
       "      <th>scroll_percentage</th>\n",
       "      <th>device_type</th>\n",
       "      <th>article_ids_inview</th>\n",
       "      <th>article_ids_clicked</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_sso_user</th>\n",
       "      <th>gender</th>\n",
       "      <th>postcode</th>\n",
       "      <th>age</th>\n",
       "      <th>is_subscriber</th>\n",
       "      <th>session_id</th>\n",
       "      <th>next_read_time</th>\n",
       "      <th>next_scroll_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-21 21:06:50</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>[9774516, 9771051, 9770028, 9775402, 9774461, ...</td>\n",
       "      <td>[9759966]</td>\n",
       "      <td>22779</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152513</td>\n",
       "      <td>9778745.0</td>\n",
       "      <td>2023-05-24 07:31:26</td>\n",
       "      <td>30.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[9778669, 9778736, 9778623, 9089120, 9778661, ...</td>\n",
       "      <td>[9778661]</td>\n",
       "      <td>150224</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>298</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-24 07:30:33</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[9778369, 9777856, 9778500, 9778021, 9778627, ...</td>\n",
       "      <td>[9777856]</td>\n",
       "      <td>160892</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>401</td>\n",
       "      <td>215.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-23 05:25:40</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>[9776715, 9776406, 9776566, 9776071, 9776808, ...</td>\n",
       "      <td>[9776566]</td>\n",
       "      <td>1001055</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1357</td>\n",
       "      <td>40.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-23 05:31:54</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>[9775202, 9776855, 9776688, 9771995, 9776583, ...</td>\n",
       "      <td>[9776553]</td>\n",
       "      <td>1001055</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1358</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  article_id     impression_time  read_time  \\\n",
       "0          48401         NaN 2023-05-21 21:06:50       21.0   \n",
       "1         152513   9778745.0 2023-05-24 07:31:26       30.0   \n",
       "2         155390         NaN 2023-05-24 07:30:33       45.0   \n",
       "3         214679         NaN 2023-05-23 05:25:40       33.0   \n",
       "4         214681         NaN 2023-05-23 05:31:54       21.0   \n",
       "\n",
       "   scroll_percentage  device_type  \\\n",
       "0                NaN            2   \n",
       "1              100.0            1   \n",
       "2                NaN            1   \n",
       "3                NaN            2   \n",
       "4                NaN            2   \n",
       "\n",
       "                                  article_ids_inview article_ids_clicked  \\\n",
       "0  [9774516, 9771051, 9770028, 9775402, 9774461, ...           [9759966]   \n",
       "1  [9778669, 9778736, 9778623, 9089120, 9778661, ...           [9778661]   \n",
       "2  [9778369, 9777856, 9778500, 9778021, 9778627, ...           [9777856]   \n",
       "3  [9776715, 9776406, 9776566, 9776071, 9776808, ...           [9776566]   \n",
       "4  [9775202, 9776855, 9776688, 9771995, 9776583, ...           [9776553]   \n",
       "\n",
       "   user_id  is_sso_user  gender  postcode  age  is_subscriber  session_id  \\\n",
       "0    22779        False     NaN       NaN  NaN          False          21   \n",
       "1   150224        False     NaN       NaN  NaN          False         298   \n",
       "2   160892        False     NaN       NaN  NaN          False         401   \n",
       "3  1001055        False     NaN       NaN  NaN          False        1357   \n",
       "4  1001055        False     NaN       NaN  NaN          False        1358   \n",
       "\n",
       "   next_read_time  next_scroll_percentage  \n",
       "0            16.0                    27.0  \n",
       "1             2.0                    48.0  \n",
       "2           215.0                   100.0  \n",
       "3            40.0                    47.0  \n",
       "4             5.0                    49.0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
