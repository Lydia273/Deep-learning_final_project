{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files:\n",
      "/zhome/7a/2/203207/.cache/huggingface/hub/models--stanfordnlp--glove/snapshots/1db2080b2d94def6e5b0386a523102f9d8849e9d/glove.6B.zip\n",
      "/zhome/7a/2/203207/.cache/huggingface/hub/models--stanfordnlp--glove/snapshots/1db2080b2d94def6e5b0386a523102f9d8849e9d/glove.6B.200d.txt\n",
      "/zhome/7a/2/203207/.cache/huggingface/hub/models--stanfordnlp--glove/snapshots/1db2080b2d94def6e5b0386a523102f9d8849e9d/glove.6B.50d.txt\n",
      "/zhome/7a/2/203207/.cache/huggingface/hub/models--stanfordnlp--glove/snapshots/1db2080b2d94def6e5b0386a523102f9d8849e9d/glove.6B.300d.txt\n",
      "/zhome/7a/2/203207/.cache/huggingface/hub/models--stanfordnlp--glove/snapshots/1db2080b2d94def6e5b0386a523102f9d8849e9d/glove.6B.100d.txt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target file `glove.840B.300d.txt` can't be found. Check if `glove.840B.300d.txt` was properly downloaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 65\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vocabulary, vectors\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m glove_vocabulary, glove_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mload_glove_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#glove voc is a list of words from the glove embeddings , glove vec is a tensor where each row represents the embedding of a word in glove voc \u001b[39;00m\n\u001b[1;32m     66\u001b[0m rich\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglove_vocabulary: type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(glove_vocabulary)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, length=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(glove_vocabulary)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m rich\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglove_vectors: type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(glove_vectors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglove_vectors\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglove_vectors\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [31], line 46\u001b[0m, in \u001b[0;36mload_glove_vectors\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39miterdir():\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28mprint\u001b[39m(p)\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget file `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be found. Check if `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` was properly downloaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m         )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Parse vocabulary and vectors\u001b[39;00m\n\u001b[1;32m     51\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: Target file `glove.840B.300d.txt` can't be found. Check if `glove.840B.300d.txt` was properly downloaded."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spmy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# %% [markdown]\n",
    "# Preparing the GloVe embeddings and tokenizer \n",
    "\n",
    "# %%\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, List\n",
    "import torch\n",
    "import tokenizers\n",
    "import rich\n",
    "\n",
    "# Load GloVe embeddings\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "from typing import List, Tuple\n",
    "\n",
    "def load_glove_vectors(filename=\"glove.840B.300d.txt\") -> Tuple[List[str], torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Load the GloVe vectors and parse vocabulary and vectors.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of the GloVe file to load.\n",
    "\n",
    "    Returns:\n",
    "        vocabulary (List[str]): List of words in the GloVe vocabulary.\n",
    "        vectors (torch.Tensor): Tensor of embedding vectors.\n",
    "    \"\"\"\n",
    "    # Download GloVe file from Hugging Face hub (ensure correct version)\n",
    "    try:\n",
    "        path = Path(hf_hub_download(repo_id=\"stanfordnlp/glove\", filename=\"glove.840B.300d.zip\"))\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Error downloading the GloVe file from Hugging Face: \" + str(e))\n",
    "    \n",
    "    # Unzip if the file exists\n",
    "    target_file = path.parent / filename\n",
    "    if not target_file.exists():\n",
    "        with zipfile.ZipFile(path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(path.parent)\n",
    "\n",
    "        # Check if file was extracted correctly\n",
    "        if not target_file.exists():\n",
    "            print(\"Available files:\")\n",
    "            for p in path.parent.iterdir():\n",
    "                print(p)\n",
    "            raise ValueError(f\"Target file `{filename}` can't be found. Check if `{filename}` was properly downloaded.\")\n",
    "    \n",
    "    # Parse vocabulary and vectors\n",
    "    vocabulary = []\n",
    "    vectors = []\n",
    "    with open(target_file, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in tqdm(f.readlines(), desc=f\"Parsing {target_file.name}...\"):\n",
    "            word, *vector = line.split()\n",
    "            vocabulary.append(word)\n",
    "            vectors.append(torch.tensor([float(v) for v in vector], dtype=torch.float32))\n",
    "    vectors = torch.stack(vectors)\n",
    "    return vocabulary, vectors\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# Prepare data\n",
    "glove_vocabulary, glove_vectors = load_glove_vectors() #glove voc is a list of words from the glove embeddings , glove vec is a tensor where each row represents the embedding of a word in glove voc \n",
    "rich.print(f\"glove_vocabulary: type={type(glove_vocabulary)}, length={len(glove_vocabulary)}\")\n",
    "rich.print(f\"glove_vectors: type={type(glove_vectors)}, shape={glove_vectors.shape}, dtype={glove_vectors.dtype}\")\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = ['<|start|>', '<|unknown|>', '<|pad|>']  # add special toeksn , useful for seq processing\n",
    "glove_vocabulary = special_tokens + glove_vocabulary # creates random embeddings for these tokens with the same dim as other glove embeddings\n",
    "glove_vectors = torch.cat([torch.randn(len(special_tokens), glove_vectors.shape[1]), glove_vectors]) # concatenates these new embeddings to the glove vectors\n",
    "\n",
    "# Tokenizer for GloVe\n",
    "glove_tokenizer = tokenizers.Tokenizer( # A WordLevel tokenizer maps each word in the vocabulary to a unique integer (token ID).\n",
    "    tokenizers.models.WordLevel(vocab={v: i for i, v in enumerate(glove_vocabulary)}, unk_token=\"<|unknown|>\") # The tokenizer uses <|unknown|> as a fallback for words not found in the vocabulary.\n",
    ")\n",
    "glove_tokenizer.normalizer = tokenizers.normalizers.BertNormalizer(strip_accents=False)\n",
    "glove_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load train and validation datasets\n",
    "dataset_train_parquet = \"final_df_train.parquet\"\n",
    "df_train = pd.read_parquet(dataset_train_parquet, engine=\"pyarrow\")\n",
    "\n",
    "dataset_validation_parquet = \"final_df_valid.parquet\"\n",
    "df_validation = pd.read_parquet(dataset_validation_parquet, engine=\"pyarrow\")\n",
    "\n",
    "# Combine train and validation titles into a single process for tokenization and padding\n",
    "all_titles = pd.concat(\n",
    "    [df_train[\"title\"].fillna(\"\"), df_validation[\"title\"].fillna(\"\")], ignore_index=True\n",
    ").tolist()\n",
    "\n",
    "\n",
    "# Define padding length\n",
    "MAX_TITLE_LENGTH = 10  # Adjust this to the max length of titles you want\n",
    "\n",
    "# Modify tokenization function to pad or truncate titles\n",
    "def tokenize_titles(titles, tokenizer, max_length=MAX_TITLE_LENGTH):\n",
    "    tokenized_titles = []\n",
    "    pad_token_id = tokenizer.token_to_id(\"<|pad|>\")  # Get the padding token ID from the tokenizer\n",
    "\n",
    "    for title in tqdm(titles, desc=\"Tokenizing titles\"):\n",
    "        encoding = tokenizer.encode(title)\n",
    "        token_ids = encoding.ids\n",
    "        token_ids = [min(id, len(glove_vocabulary) - 1) for id in token_ids]  # Clip the IDs if they are out of range\n",
    "\n",
    "        # Truncate or pad the tokenized title to max_length\n",
    "        if len(token_ids) < max_length:\n",
    "            # Pad to the right\n",
    "            token_ids += [pad_token_id] * (max_length - len(token_ids))\n",
    "        else:\n",
    "            # Truncate the sequence\n",
    "            token_ids = token_ids[:max_length]\n",
    "\n",
    "        tokenized_titles.append(token_ids)\n",
    "\n",
    "    return tokenized_titles\n",
    "\n",
    "# Tokenize and pad all titles\n",
    "tokenized_titles = tokenize_titles(all_titles, glove_tokenizer, max_length=MAX_TITLE_LENGTH)\n",
    "\n",
    "# Convert tokenized titles to tensor\n",
    "tokenized_titles_tensor = torch.tensor(tokenized_titles, dtype=torch.long)\n",
    "\n",
    "# You can now use tokenized_titles_tensor for embedding input\n",
    "\n",
    "\n",
    "# You can now use tokenized_titles_tensor for embedding input\n",
    "\n",
    "\n",
    "# Step 3: Prepare user history and candidate news\n",
    "def prepare_data(df):\n",
    "    user_histories = []\n",
    "    candidate_news = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Preparing data\"):\n",
    "        # Assuming 'article_ids_clicked' contains a list of article IDs user has clicked\n",
    "        user_history = row['article_ids_clicked']  # This should be a list of IDs\n",
    "        candidate_news_id = row['article_id']  # Current article ID for prediction\n",
    "        \n",
    "        # If the history is less than the expected length, pad with -1 (or any other padding value)\n",
    "        user_history_padded = user_history[:10] + [-1] * (10 - len(user_history))  # Max length 10 for history\n",
    "        \n",
    "        # Append the history and candidate news\n",
    "        user_histories.append(user_history_padded)\n",
    "        candidate_news.append([candidate_news_id] * 20)  # Assuming you have 20 candidate news articles\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    user_histories_tensor = torch.tensor(user_histories, dtype=torch.long)\n",
    "    candidate_news_tensor = torch.tensor(candidate_news, dtype=torch.long)\n",
    "\n",
    "    return user_histories_tensor, candidate_news_tensor\n",
    "\n",
    "# Prepare data for train set\n",
    "train_user_history, train_candidate_news = prepare_data(df_train)\n",
    "\n",
    "# Prepare data for validation set\n",
    "valid_user_history, valid_candidate_news = prepare_data(df_validation)\n",
    "\n",
    "# Check shapes (optional)\n",
    "print(\"Train user history shape:\", train_user_history.shape)\n",
    "print(\"Train candidate news shape:\", train_candidate_news.shape)\n",
    "print(\"Validation user history shape:\", valid_user_history.shape)\n",
    "print(\"Validation candidate news shape:\", valid_candidate_news.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Word Embedding Layer\n",
    "class WordEmbeddings(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(WordEmbeddings, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embeddings=torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=True  # Set to False to fine-tune embeddings\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Multi-Head Self-Attention Layer (Vectorized)\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, and V\n",
    "        self.query_projection = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_projection = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_projection = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_projection = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        Returns:\n",
    "            output (torch.Tensor): Output tensor of shape (batch_size, seq_len, embed_dim)\n",
    "            attention_weights (torch.Tensor): Attention weights of shape (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = inputs.size()\n",
    "\n",
    "        # Project inputs to queries, keys, and values\n",
    "        Q = self.query_projection(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        K = self.key_projection(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        V = self.value_projection(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Reshape to separate heads\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # Normalize over seq_len\n",
    "\n",
    "        # Compute context\n",
    "        context = torch.matmul(attention_weights, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Concatenate heads and project the output\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.out_projection(context)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# Additive Word Attention Layer\n",
    "class AdditiveWordAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, query_dim):\n",
    "        super(AdditiveWordAttention, self).__init__()\n",
    "        self.Vw = nn.Linear(embed_dim, query_dim, bias=True)\n",
    "        self.qw = nn.Parameter(torch.randn(query_dim))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        Returns:\n",
    "            weighted_sum (torch.Tensor): Tensor of shape (batch_size, embed_dim)\n",
    "            attention_weights (torch.Tensor): Tensor of shape (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        projection = self.Vw(inputs)  # (batch_size, seq_len, query_dim)\n",
    "        scores = torch.tanh(projection)  # Non-linear transformation\n",
    "        attention_scores = torch.matmul(scores, self.qw)  # (batch_size, seq_len)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # Normalize over seq_len\n",
    "        weighted_sum = torch.sum(inputs * attention_weights.unsqueeze(-1), dim=1)  # Weighted sum of embeddings\n",
    "\n",
    "        return weighted_sum, attention_weights\n",
    "\n",
    "\n",
    "# News Encoder\n",
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim, num_heads, query_dim, dropout=0.2):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(\n",
    "            embedding_matrix.clone().detach(), freeze=False\n",
    "        )\n",
    "        self.self_attention = MultiHeadSelfAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.additive_attention = AdditiveWordAttention(embed_dim=embed_dim, query_dim=query_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input tensor of shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            news_representation (torch.Tensor): Tensor of shape (batch_size, embed_dim)\n",
    "            attention_weights (torch.Tensor): Tensor of shape (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        x = self.word_embeddings(inputs)  # Shape: (batch_size, seq_len, embed_dim)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Multi-Head Self-Attention\n",
    "        attn_output, _ = self.self_attention(x)\n",
    "\n",
    "        # Additive Attention\n",
    "        news_representation, attention_weights = self.additive_attention(attn_output)\n",
    "\n",
    "        return news_representation, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['read_time', 'article_id', 'article_ids_clicked', 'user_id',\n",
       "       'is_sso_user', 'is_subscriber', 'session_id', 'next_read_time',\n",
       "       'next_scroll_percentage', 'impr_pub_interval_milliseconds',\n",
       "       'impr_pub_hour_interval', 'title', 'time_interval',\n",
       "       'subcategory_encoded', 'total_inviews', 'total_pageviews',\n",
       "       'total_read_time', 'sentiment_score', 'impression_time_fixed',\n",
       "       'scroll_percentage_fixed', 'article_id_fixed', 'read_time_fixed',\n",
       "       'sentiment_Negative', 'sentiment_Neutral', 'sentiment_Positive'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements multi-head additive attention for user encoding,\n",
    "    strictly following the provided formulas.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - embed_dim: Dimensionality of the input embeddings (news representations).\n",
    "        - num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAdditiveAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Parameters for each head\n",
    "        self.Q_n = nn.ParameterList([nn.Parameter(torch.randn(self.head_dim, self.head_dim)) for _ in range(num_heads)])\n",
    "        self.V_n = nn.ParameterList([nn.Parameter(torch.randn(self.head_dim, self.head_dim)) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, news_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - news_embeddings: Tensor of shape (batch_size, num_news, embed_dim),\n",
    "                           representing the news representations.\n",
    "\n",
    "        Returns:\n",
    "        - enhanced_news_embeddings: Tensor of shape (batch_size, num_news, embed_dim),\n",
    "                                    enhanced news representations.\n",
    "        - attention_weights: List of tensors of shape (batch_size, num_news, num_news) per head,\n",
    "                             attention weights for each head.\n",
    "        \"\"\"\n",
    "        batch_size, num_news, embed_dim = news_embeddings.size()\n",
    "\n",
    "        # Split embeddings for each head\n",
    "        news_per_head = news_embeddings.view(batch_size, num_news, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        all_head_outputs = []\n",
    "        all_attention_weights = []\n",
    "\n",
    "        for h in range(self.num_heads):\n",
    "            Q_n = self.Q_n[h]\n",
    "            scores = torch.einsum('bnd,dk,bmk->bnm', news_per_head[:, h, :, :], Q_n, news_per_head[:, h, :, :])\n",
    "            attention_weights = F.softmax(scores, dim=-1)\n",
    "            V_n = self.V_n[h]\n",
    "            head_output = torch.einsum('bnm,bmd,dk->bnd', attention_weights, news_per_head[:, h, :, :], V_n)\n",
    "\n",
    "            all_head_outputs.append(head_output)\n",
    "            all_attention_weights.append(attention_weights)\n",
    "\n",
    "        concat_output = torch.cat(all_head_outputs, dim=-1)\n",
    "        return concat_output, all_attention_weights\n",
    "    \n",
    "\n",
    "class UserAdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements additive attention for user encoding based on the provided formulas.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - embed_dim: Dimensionality of the input embeddings (news representations).\n",
    "        \"\"\"\n",
    "        super(UserAdditiveAttention, self).__init__()\n",
    "        self.V_n = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_n = nn.Parameter(torch.zeros(embed_dim))\n",
    "        self.q_n = nn.Parameter(torch.randn(embed_dim))\n",
    "\n",
    "    def forward(self, news_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - news_embeddings: Tensor of shape (batch_size, num_news, embed_dim),\n",
    "                           representing the news representations.\n",
    "\n",
    "        Returns:\n",
    "        - user_representation: Tensor of shape (batch_size, embed_dim),\n",
    "                               the final user representation.\n",
    "        - attention_weights: Tensor of shape (batch_size, num_news),\n",
    "                             attention weights for the news articles.\n",
    "        \"\"\"\n",
    "        transformed_news = self.V_n(news_embeddings)\n",
    "        scores = torch.tanh(transformed_news + self.v_n)\n",
    "        scores = torch.einsum('bnd,d->bn', scores, self.q_n)\n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        user_representation = torch.einsum('bn,bnd->bd', attention_weights, news_embeddings)\n",
    "        return user_representation, attention_weights\n",
    "    \n",
    "\n",
    "class UserEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines MultiHeadAdditiveAttention and UserAdditiveAttention\n",
    "    to encode user representations based on news embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - embed_dim: Dimensionality of the input embeddings (news representations).\n",
    "        - num_heads: Number of attention heads in the MultiHeadAdditiveAttention layer.\n",
    "        \"\"\"\n",
    "        super(UserEncoder, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAdditiveAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.user_attention = UserAdditiveAttention(embed_dim=embed_dim)\n",
    "\n",
    "    def forward(self, news_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - news_embeddings: Tensor of shape (batch_size, num_news, embed_dim),\n",
    "                           representing the news representations.\n",
    "\n",
    "        Returns:\n",
    "        - user_representation: Tensor of shape (batch_size, embed_dim),\n",
    "                               the final user representation.\n",
    "        - attention_weights: Dictionary containing:\n",
    "            - 'multi_head_attention': List of tensors of shape (batch_size, num_news, num_news) per head,\n",
    "                                      attention weights for each head from the MultiHeadAdditiveAttention layer.\n",
    "            - 'user_attention': Tensor of shape (batch_size, num_news),\n",
    "                                attention weights for the news articles from the UserAdditiveAttention layer.\n",
    "        \"\"\"\n",
    "        enhanced_news_embeddings, multi_head_attention_weights = self.multi_head_attention(news_embeddings)\n",
    "        user_representation, user_attention_weights = self.user_attention(enhanced_news_embeddings)\n",
    "\n",
    "        attention_weights = {\n",
    "            'multi_head_attention': multi_head_attention_weights,\n",
    "            'user_attention': user_attention_weights\n",
    "        }\n",
    "        return user_representation, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Representations Shape: torch.Size([5, 300])\n",
      "Candidate News Representations Shape: torch.Size([5, 300])\n",
      "Click Probabilities Shape: torch.Size([5, 1])\n",
      "Click Probabilities:\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ClickPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Click Predictor Module: Predicts the probability of a user clicking on a candidate news article.\n",
    "    Uses a dot product between the user representation and the candidate news representation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ClickPredictor, self).__init__()\n",
    "        # No additional parameters are needed; the click probability\n",
    "        # is computed using the dot product followed by a sigmoid activation.\n",
    "    \n",
    "    def forward(self, user_repr, candidate_news_repr):\n",
    "        \"\"\"\n",
    "        Forward pass for click prediction.\n",
    "        \n",
    "        Args:\n",
    "            user_repr (Tensor): User representation tensor of shape (batch_size, embedding_dim).\n",
    "            candidate_news_repr (Tensor): Candidate news representation tensor of shape (batch_size, embedding_dim).\n",
    "    \n",
    "        Returns:\n",
    "            click_prob (Tensor): Click probabilities tensor of shape (batch_size, 1), with values in [0, 1].\n",
    "        \"\"\"\n",
    "        # Ensure the embedding dimensions match\n",
    "        assert user_repr.size(1) == candidate_news_repr.size(1), \"Embedding dimensions must match!\"\n",
    "        \n",
    "        # Step 1: Compute the dot product between user and candidate news representations\n",
    "        # This measures the similarity between user interests and news content\n",
    "        click_score = torch.sum(user_repr * candidate_news_repr, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "    \n",
    "        # Step 2: Apply a sigmoid activation function to get probabilities in [0, 1]\n",
    "        click_prob = torch.sigmoid(click_score)  # Shape: (batch_size, 1)\n",
    "    \n",
    "        # Step 3: Return the click probabilities\n",
    "        return click_prob\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate example inputs\n",
    "    batch_size = 5      # Number of user-news pairs in a batch\n",
    "    embedding_dim = 300 # Embedding dimension matching the NewsEncoder and UserEncoder outputs\n",
    "    \n",
    "    # Random user and news embeddings\n",
    "    user_repr = torch.rand(batch_size, embedding_dim)          # Shape: (batch_size, embedding_dim)\n",
    "    candidate_news_repr = torch.rand(batch_size, embedding_dim) # Shape: (batch_size, embedding_dim)\n",
    "    \n",
    "    # Instantiate the Click Predictor\n",
    "    click_predictor = ClickPredictor()\n",
    "    \n",
    "    # Forward pass: Compute click probabilities\n",
    "    click_probs = click_predictor(user_repr, candidate_news_repr)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"User Representations Shape:\", user_repr.shape)\n",
    "    print(\"Candidate News Representations Shape:\", candidate_news_repr.shape)\n",
    "    print(\"Click Probabilities Shape:\", click_probs.shape)\n",
    "    print(\"Click Probabilities:\\n\", click_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NRMS(nn.Module):\n",
    "    \"\"\"\n",
    "    NRMS Model: News Recommendation Model using Multi-Head Additive Attention\n",
    "    and Click Prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, embed_dim, num_heads, query_dim, dropout=0.2):\n",
    "        super(NRMS, self).__init__()\n",
    "        \n",
    "        # News Encoder: Encodes the news articles\n",
    "        self.news_encoder = NewsEncoder(\n",
    "            embedding_matrix=embedding_matrix,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            query_dim=query_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # User Encoder: Encodes the user based on news interactions\n",
    "        self.user_encoder = UserEncoder(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        \n",
    "        # Click Predictor: Predicts the likelihood of a click based on user and news representations\n",
    "        self.click_predictor = ClickPredictor()\n",
    "\n",
    "    def forward(self, user_history, candidate_news):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_history (Tensor): Tensor of shape (batch_size, history_len),\n",
    "                                    representing the IDs of news articles that the user has interacted with.\n",
    "            candidate_news (Tensor): Tensor of shape (batch_size, num_news),\n",
    "                                      representing the candidate news articles to predict clicks for.\n",
    "        \n",
    "        Returns:\n",
    "            click_prob (Tensor): Tensor of shape (batch_size, 1), representing the click probabilities.\n",
    "        \"\"\"\n",
    "        # Step 1: Get news representations\n",
    "        news_repr, _ = self.news_encoder(candidate_news)  # Get the representation of the candidate news\n",
    "\n",
    "        # Step 2: Get user representation\n",
    "        user_repr, _ = self.user_encoder(user_history)  # Get the representation of the user based on their history\n",
    "        \n",
    "        # Step 3: Predict click probability\n",
    "        click_prob = self.click_predictor(user_repr, news_repr)  # Get the click probability\n",
    "\n",
    "        return click_prob\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the embedding matrix (this should be pre-trained word embeddings)\n",
    "    embedding_matrix = torch.rand(10000, 300)  # Example: 10,000 words, each with 300-dimensional embeddings\n",
    "    \n",
    "    # Parameters\n",
    "    batch_size = 5        # Number of users in a batch\n",
    "    history_len = 10      # Number of news articles in user history\n",
    "    num_news = 20         # Number of candidate news articles for each user\n",
    "    embed_dim = 300       # Embedding dimension\n",
    "    num_heads = 8         # Number of attention heads\n",
    "    query_dim = 64        # Dimension for additive attention\n",
    "    \n",
    "    # Simulate example inputs\n",
    "    user_history = torch.randint(0, 10000, (batch_size, history_len))  # User history (batch_size, history_len)\n",
    "    candidate_news = torch.randint(0, 10000, (batch_size, num_news))  # Candidate news (batch_size, num_news)\n",
    "    \n",
    "    # Instantiate the NRMS model\n",
    "    nrms_model = NRMS(\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        query_dim=query_dim,\n",
    "        dropout=0.2\n",
    "    )\n",
    "    \n",
    "    # Forward pass: Compute click probabilities\n",
    "    click_probs = nrms_model(user_history, candidate_news)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"User History Shape:\", user_history.shape)\n",
    "    print(\"Candidate News Shape:\", candidate_news.shape)\n",
    "    print(\"Click Probabilities Shape:\", click_probs.shape)\n",
    "    print(\"Click Probabilities:\\n\", click_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Update hyperparameters based on the provided values\n",
    "embed_dim = 300          # Dimension of word embeddings (GloVe)\n",
    "num_heads = 15         # Number of self-attention heads\n",
    "head_dim = 16            # Each attention head output dimension\n",
    "query_dim = 200          # Dimension of additive attention query\n",
    "dropout = 0.2            # 20% dropout on embeddings\n",
    "learning_rate = 0.001    # Learning rate for Adam optimizer\n",
    "batch_size = 64          # Batch size for training\n",
    "negative_sampling_ratio = 4  # Negative sampling ratio K\n",
    "epochs = 10              # Number of training epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NRMS(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim, num_heads, query_dim, dropout=0.2):\n",
    "        super(NRMS, self).__init__()\n",
    "        self.news_encoder = NewsEncoder(embedding_matrix=embedding_matrix, embed_dim=embed_dim, num_heads=num_heads, query_dim=query_dim, dropout=dropout)\n",
    "        self.user_encoder = UserEncoder(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.click_predictor = ClickPredictor()\n",
    "\n",
    "    def forward(self, user_history, candidate_news):\n",
    "        news_repr, _ = self.news_encoder(candidate_news)\n",
    "        user_repr, _ = self.user_encoder(user_history)\n",
    "        click_prob = self.click_predictor(user_repr, news_repr)\n",
    "        return click_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token ID in user history: 9759965\n",
      "Max token ID in candidate news: 9774516\n",
      "Vocabulary size: 400004\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "User history contains out-of-vocabulary tokens!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(glove_vocabulary))  \u001b[38;5;66;03m# Ensure this matches\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Assert that the max token IDs are within bounds\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_user_history\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(glove_vocabulary), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser history contains out-of-vocabulary tokens!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_candidate_news\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(glove_vocabulary), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCandidate news contains out-of-vocabulary tokens!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: User history contains out-of-vocabulary tokens!"
     ]
    }
   ],
   "source": [
    "# Custom tokenizer that handles OOV tokens\n",
    "def tokenize_with_unk(text, tokenizer, unk_token_id=1):\n",
    "    # Tokenize the text using the tokenizer\n",
    "    encoding = tokenizer.encode(text)\n",
    "    token_ids = encoding.ids  # Token IDs from the tokenizer\n",
    "    \n",
    "    # Replace any token not found in the vocabulary with the `<|unknown|>` token ID\n",
    "    token_ids = [token_id if token_id in tokenizer.get_vocab() else unk_token_id for token_id in token_ids]\n",
    "    \n",
    "    return token_ids\n",
    "\n",
    "# Clip token IDs to ensure they are within the bounds of the vocabulary size\n",
    "def clip_token_ids(tensor, vocab_size, unk_token_id=1):\n",
    "    # Ensure no index is out of range\n",
    "    tensor = torch.clamp(tensor, min=0, max=vocab_size - 1)\n",
    "    \n",
    "    # If any token ID is out of bounds, replace it with the unknown token ID\n",
    "    tensor[tensor >= vocab_size] = unk_token_id\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# Modify the `prepare_data` function accordingly\n",
    "def prepare_data(df, vocab_size, tokenizer, unk_token_id=1):\n",
    "    user_histories = []\n",
    "    candidate_news = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Preparing data\"):\n",
    "        user_history = row['article_ids_clicked']  # List of clicked article IDs\n",
    "        candidate_news_id = row['article_id']  # Current candidate article ID\n",
    "        \n",
    "        # Tokenize user history: Assuming `article_ids_clicked` contains text that needs tokenization\n",
    "        user_history_token_ids = [tokenize_with_unk(article_id, tokenizer, unk_token_id) for article_id in user_history]\n",
    "\n",
    "        # Pad user history if it's shorter than the required length (max length = 10 for example)\n",
    "        user_history_padded = user_history_token_ids[:10] + [unk_token_id] * (10 - len(user_history_token_ids))  # Max length 10 for history\n",
    "\n",
    "        # Clip user history to be within bounds of the vocabulary\n",
    "        user_history_padded = clip_token_ids(torch.tensor(user_history_padded, dtype=torch.long), vocab_size, unk_token_id)\n",
    "\n",
    "        # Tokenize and pad candidate news article IDs\n",
    "        candidate_news_token_id = tokenize_with_unk(candidate_news_id, tokenizer, unk_token_id)\n",
    "        candidate_news_padded = [candidate_news_token_id] * 20  # Assuming you have 20 candidate news articles\n",
    "        candidate_news_padded = clip_token_ids(torch.tensor(candidate_news_padded, dtype=torch.long), vocab_size, unk_token_id)\n",
    "\n",
    "        # Append the history and candidate news\n",
    "        user_histories.append(user_history_padded)\n",
    "        candidate_news.append(candidate_news_padded)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    user_histories_tensor = torch.stack(user_histories)\n",
    "    candidate_news_tensor = torch.stack(candidate_news)\n",
    "\n",
    "    return user_histories_tensor, candidate_news_tensor\n",
    "\n",
    "# Ensure the token IDs are within the correct range\n",
    "print(\"Max token ID in user history:\", train_user_history.max().item())\n",
    "print(\"Max token ID in candidate news:\", train_candidate_news.max().item())\n",
    "print(\"Vocabulary size:\", len(glove_vocabulary))  # Ensure this matches\n",
    "\n",
    "# Assert that the max token IDs are within bounds\n",
    "assert train_user_history.max().item() < len(glove_vocabulary), \"User history contains out-of-vocabulary tokens!\"\n",
    "assert train_candidate_news.max().item() < len(glove_vocabulary), \"Candidate news contains out-of-vocabulary tokens!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max user history token ID: 9759965\n",
      "Vocabulary size: 400004\n"
     ]
    }
   ],
   "source": [
    "# Check the max token ID in train_user_history\n",
    "print(\"Max user history token ID:\", train_user_history.max().item())\n",
    "print(\"Vocabulary size:\", len(glove_vocabulary))  # Check the vocabulary size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero gradients before backward pass\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass: Compute click probabilities\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m click_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_user_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_candidate_news\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Assuming you have a column for actual clicks: 'clicked' or similar (1 for clicked, 0 for not clicked)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m click_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclicked\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust if necessary\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn [8], line 9\u001b[0m, in \u001b[0;36mNRMS.forward\u001b[0;34m(self, user_history, candidate_news)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, user_history, candidate_news):\n\u001b[0;32m----> 9\u001b[0m     news_repr, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnews_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_news\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     user_repr, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_encoder(user_history)\n\u001b[1;32m     11\u001b[0m     click_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclick_predictor(user_repr, news_repr)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn [3], line 217\u001b[0m, in \u001b[0;36mNewsEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m        inputs (torch.Tensor): Input tensor of shape (batch_size, seq_len)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m        attention_weights (torch.Tensor): Tensor of shape (batch_size, seq_len)\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (batch_size, seq_len, embed_dim)\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# Multi-Head Self-Attention\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Initialize model with GloVe embeddings (assume glove_vectors are loaded as done previously)\n",
    "embedding_matrix = glove_vectors\n",
    "model = NRMS(embedding_matrix=embedding_matrix, embed_dim=embed_dim, num_heads=num_heads, query_dim=query_dim, dropout=dropout)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for click prediction\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()  # Zero gradients before backward pass\n",
    "    \n",
    "    # Forward pass: Compute click probabilities\n",
    "    click_probs = model(train_user_history, train_candidate_news)\n",
    "    \n",
    "    # Assuming you have a column for actual clicks: 'clicked' or similar (1 for clicked, 0 for not clicked)\n",
    "    click_labels = torch.tensor(df_train['clicked'].values, dtype=torch.float32).unsqueeze(1)  # Adjust if necessary\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(click_probs, click_labels)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
